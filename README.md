# ğŸ”Š NeuraVoice â€“ Real-Time Voice Assistant (WIP)

> A full-stack AI voice assistant platform for **real-time audio/text interaction**, powered by **Groq + LLaMA3**, **Whisper STT**, and **ElevenLabs TTS**, with dynamic WebSocket communication and a multimodal UI.

---

## ğŸ›ï¸ Problem Statement

While LLM chat interfaces are common, real-time **voice-based agents** are rare, especially those that:

- Handle both **audio and text input**
- Offer **audio or text output**, user-configurable
- Work over **WebSockets for low latency**
- Offer a clean UI + backend structure

Most assistants either:
- Are locked behind closed APIs,
- Or lack proper integration between voice and reasoning models.

---

## âœ… Our Solution

We built **NeuraVoice** â€” a real-time, audio-optional assistant that features:

- ğŸ§ **Voice input** via browser microphone
- ğŸ”Š **Audio or text responses** (configurable)
- ğŸ§  Powered by **Groq LLaMA3**, **Whisper**, and **ElevenLabs**
- âœ¨ **Interactive React frontend** with chat UI
- â³ Uses **WebSockets** for low-latency real-time flow

Built with modularity in mind, this can be the base for agentic AI systems.

---

## ğŸš€ Features

| Component     | Description                                                                 |
|---------------|-----------------------------------------------------------------------------|
| ğŸ“» Audio Chat   | Press mic button to record 5s audio; transcribed and processed in real-time |
| ğŸ’¬ Text Chat    | Type text and receive replies like a traditional chat app         |
| ğŸ”„ Dual Modes   | Choose whether AI replies in text or audio                         |
| ğŸ” STT + TTS    | Combines Whisper (STT) and ElevenLabs (TTS)                        |
| âœ¨ Fast Response | Uses WebSocket for instant audio/text streaming                        |
| ğŸŒ Deploy-ready | Easy to host, configure, and personalize                          |

---

## ğŸ› ï¸ Tech Stack

- **Frontend**: React (JSX), HTML5 Audio APIs
- **Backend**: FastAPI, WebSockets
- **LLM**: Groq API (LLaMA3-8B)
- **TTS**: ElevenLabs
- **STT**: OpenAI Whisper
- **Other**: dotenv, tempfile, asyncio, base64, BytesIO

---

## ğŸ“ Folder Structure

```bash
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py               # FastAPI entry
â”‚   â”œâ”€â”€ ws_routes.py          # WebSocket endpoint for audio/text
â”‚   â”œâ”€â”€ phiagent2_groq.py     # Calls Groq API
â”‚   â”œâ”€â”€ list_voices.py        # Optional helper for ElevenLabs voices
â”‚   â””â”€â”€ requirements.txt      # Python dependencies
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ AudioRecorder.jsx # React component for mic/text input
â”‚       â”œâ”€â”€ AudioRecorder.css # Styling
â”‚       â”œâ”€â”€ App.js, index.js  # App entrypoints
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ .env                      # API keys
â”œâ”€â”€ README.md
â””â”€â”€ venv/                     # Virtual env (ignored)

```

## ğŸ› ï¸ Setup Instructions

Follow these steps to run the app locally:

## 1. Clone the Repo

```
bash
git clone https://github.com/NikunjBuddhawar/NeuraVoice
cd neura-voice
```


## 2. Backend Setup (Python)

```
bash
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## 3. Add the .env file

```
ELEVENLABS_API_KEY=your-elevenlabs-key
GROQ_API_KEY=your-groq-key
```

## 4. Run the FastAPI server

```
bash
uvicorn main:app --reload
```

## 5. Frontend Setup (React)

```
bash
cd frontend
npm install
npm start
```
## ğŸ”„ Workflow Diagram

```mermaid
graph TD
    A[User Speaks or Types] --> B[Frontend (React)]
    B --> C[WebSocket /ws/audio]
    C --> D[FastAPI Backend]
    D -->|Audio| E[Whisper Transcription]
    D -->|Text| F[Groq LLM]
    E --> F
    F -->|Response| G[ElevenLabs TTS or Text]
    G --> H[Send Response Back via WebSocket]
    H --> I[Frontend Renders in Chat]
```